{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Notebook setup\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "interactive_figures = True\n",
    "if interactive_figures:\n",
    "    %matplotlib widget\n",
    "    figsize = (9,3)\n",
    "else:\n",
    "    figsize = (13,4)\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from util import nab\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filling Values Using a Model\n",
    "\n",
    "**If we want to fill missing values better, we need to _predict_ them**\n",
    "\n",
    "We need a _model_, which can infer their value\n",
    "\n",
    "* All the approaches seen so far can be considered (extremely simple) models\n",
    "* ...We just need a more advanced one!\n",
    "\n",
    "**What are the _desired properties_ of the model we seek?**\n",
    "\n",
    "Given a gap (i.e. one or more contiguous missing values), the model:\n",
    "\n",
    "* Must be able to make a prediction about the missing values\n",
    "* ...Which is consistent with all the available observations\n",
    "* I.e. it should be able to _interpolate_ the data (in generalized sense)\n",
    "\n",
    "**Most ML models _cannot_ be used for filling (in a straightforward fashion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filling Values Using a Model\n",
    "\n",
    "**Density estimation does not (natively) provide predictions**\n",
    "\n",
    "To be fair, predictions can be _extracted_ from a density estimator:\n",
    "\n",
    "- Given an estimator $f({\\bf x}, \\theta)$ for $P({\\bf x})$ we can find the most likely value for $\\bf x$ by solving:\n",
    "$$\n",
    "\\text{argmax}_{\\bf x} f({\\bf x}, \\theta)\n",
    "$$\n",
    "- This is a _Maximum A Posteriori (MAP)_\n",
    "- ...And its what most regressors/classifiers natively compute\n",
    "\n",
    "However, with a density estimator, computing the MAP can be _very expensive_\n",
    "\n",
    "**Auto-regressors makes use of past observations, but not the future ones**\n",
    "\n",
    "* They are designed for _extrapolation_ (predict beyond the boundaries)\n",
    "* ...And not for _interpolation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "**One of the few viable ML models is given by _Gaussian Processes (GP)_**\n",
    "\n",
    "We will introduce their key concepts via an example:\n",
    "\n",
    "<center><img src=\"assets/rain1.png\" width=400px/></center>\n",
    "\n",
    "* Say we want to model how rainfall changes over a stretch of land\n",
    "* $y =$ rainfall, $(x_0, x_1) =$ position on the surface of land "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "**One of the few viable ML models is given by _Gaussian Processes (GP)_**\n",
    "\n",
    "Since this is a physical phenomenon...\n",
    "\n",
    "<center><img src=\"assets/rain1.png\" width=400px/></center>\n",
    "\n",
    "* ...We can reasonably assume that $y$ is Normally distributed\n",
    "* But unless we know more, we can say nothing else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "**One of the few viable ML models is given by _Gaussian Processes (GP)_**\n",
    "\n",
    "However, if we have a few measurements...\n",
    "\n",
    "<center><img src=\"assets/rain2.png\" width=400px/></center>\n",
    "\n",
    "We can assume that _rainfall in nearby locations is similar_\n",
    "\n",
    "* We can rely on this to estimate _the chance of the measurements themselves_\n",
    "* ...And of rainfall at _positions for which we lack measurements_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "**One of the few viable ML models is given by _Gaussian Processes (GP)_**\n",
    "\n",
    "We view the measurements as components of a variable $y_X = (y_0, y_1, y_2)$\n",
    "\n",
    "<center><img src=\"assets/rain3.png\" width=400px/></center>\n",
    "\n",
    "* $y_X$ will follow a multivariate Normal distribution\n",
    "* ...And the covariance will depend on the distance between measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "**Formally:**\n",
    "\n",
    "A GP is a _stochastic process_, i.e. a collection of indexed random variables\n",
    "\n",
    "* Each variable $y_{x}$ is indexed via a tuple $x$ (e.g. location, time...)\n",
    "* The index is _continuous_ and the collection _infinite_\n",
    "* Every finite subset of $y_{x}$ variables follows a _Multivariate Normal Distribution_\n",
    "\n",
    "Some examples:\n",
    "\n",
    "* $y_{x}$ could be the rainfall rate at location $x$\n",
    "* $y_{x}$ could be the twitter volume at time $x$\n",
    "* $y_{x}$ could be the traffic volume at time $x$\n",
    "\n",
    "**In general $y_{x}$ is the value of a (stochastic) function for input $x$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multivariate Normal Distritbuion\n",
    "\n",
    "**The multivariate normal distribution?**\n",
    "\n",
    "* It works for many real world phenomena\n",
    "* It has a [closed-form density function](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) that is (relatively) easy to compute\n",
    "* The PDF is defined via a (vector) mean $\\bf \\mu$ and a covariance matrix $\\Sigma$\n",
    "* ...And often we can assume ${\\bf \\mu} = {\\bf 0}$, so _knowing $\\Sigma$ is enough_\n",
    "\n",
    "**So, given a set of indexes/input values of interest $X$**\n",
    "\n",
    "Then, if we manage to _know $\\Sigma$_, we can compute:\n",
    "\n",
    "* The probability _density_ $f({\\bf \\hat{y}_X})$ of some given observations\n",
    "  - I.e. the probability of a dataset\n",
    "* The _conditional density_ $f(\\hat{y}_x \\mid {\\bf \\hat{y}_{X}})$ of a new observation\n",
    "  - I.e. a prediction w.r.t. a set of know observations $\\hat{y}_{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining the Covariance Matrix\n",
    "\n",
    "**How do we define $\\Sigma$?**\n",
    "\n",
    "We assume that the _covariance depends on $x$_ (and not on the $y$)\n",
    "\n",
    "* Given two variables $y_{x_i}$ and $y_{x_j}$, their covariance is given by $K(x_i, x_j)$\n",
    "* Where $K$ is called a _kernel function_ (and is user chosen)\n",
    "\n",
    "**Given any finite set of variables $\\{y_{x_1}, \\ldots y_{x_n}\\}$, the covariance matrix is:**\n",
    "\n",
    "$$\\Sigma = \\left(\\begin{array}{cccc}\n",
    "K(x_1, x_1) & K(x_1, x_2) & \\cdots & K(x_1, x_n) \\\\\n",
    "K(x_2, x_1) & K(x_2, x_2) & \\cdots & K(x_2, x_n) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "K(x_n, x_1) & K(x_n, x_2) & \\cdots & K(x_n, x_n) \\\\\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "* I.e. it's entirely specified via the kernel\n",
    "\n",
    "Unfortunately, choosing the kernel completely by hand would still be _too difficult_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitting a Gaussian Process\n",
    "\n",
    "**In practice, to define the kernel we:**\n",
    "\n",
    "* Pick a _parameterized_ kernel function $K_{\\theta}(x_i, x_j)$, where $\\theta$ = parameter vector\n",
    "* Collect training observations $\\bf \\hat{y}_X$\n",
    "\n",
    "**Then we choose $\\theta$ so as to maximize the _likelihood_ of the training data, i.e.:**\n",
    "\n",
    "$$\n",
    "\\text{argmax}_\\theta f({\\bf \\hat{y}_X}, \\theta)\n",
    "$$\n",
    "\n",
    "* Where $f({\\bf \\hat{y}_X}, \\theta)$ is the _estimated_ probability density of the observations\n",
    "\n",
    "**The training problem**\n",
    "\n",
    "* Is a (possibly challenging) numerical optimization problem\n",
    "* ...Which is typically solved to _local optimality_ (e.g. via gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**Let's see how to use Gaussian Processes in scikit-learn**\n",
    "\n",
    "First, let us choose a target function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c681b943a4a908cb5b745b258ede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = lambda x: x * np.sin(2*np.pi*x) + x # target function\n",
    "x = np.linspace(0, 3, 1000)\n",
    "y = pd.Series(index=x, data=f(x))\n",
    "nab.plot_gp(target=y, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**Let's see how to use Gaussian Processes in scikit-learn**\n",
    "\n",
    "Then we build a small training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf40e022415482cac0081a80bc39999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_tr = 15\n",
    "x_tr = np.linspace(0.2, 2.8, n_tr) + 0.2*np.random.rand(n_tr)\n",
    "x_tr.sort()\n",
    "y_tr = pd.Series(index=x_tr, data=f(x_tr) + 0.2*np.random.rand(n_tr))\n",
    "nab.plot_gp(target=y, samples=y_tr, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**Let's see how to use Gaussian Processes in scikit-learn**\n",
    "\n",
    "Then we need to choose a kernel\n",
    "\n",
    "* There are [many available options](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "* We will _start_ with a simple Radial Basis Function (i.e. Gaussian) kernel\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = e^{-\\frac{d(x_i, x_j)^2}{2l}}\n",
    "$$\n",
    "\n",
    "**The correlation _decreases with the (Euclidean) distance_ $d(x_i, x_j)$:**\n",
    "\n",
    "* Intuitively, _the closer the points, the higher the correlation_\n",
    "* The $l$ parameter (_scale_) control the rate of the reduction\n",
    "\n",
    "**We have 15 indexes, so the kernel will define a $15\\times15$ covariance matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**Here's how to use an RBF kernel in scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = RBF(1, (1e-3, 1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The RBF kernel has a single parameter, representing its _scale_**\n",
    "\n",
    "The extra (tuple) parameter represents a pair of _bounds_\n",
    "\n",
    "* During training only parameter values within the boundaries will be considered\n",
    "* Bounds can be very useful for controlling the training process\n",
    "* ...Based on the available domain information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**Now we can train a Gaussian Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RBF(length_scale=0.229)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(y_tr.index.values.reshape(-1,1), y_tr.values) # needs 2D input\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training uses Gradient Descent...\n",
    "* ...To maximize the likelihood (density) of the training data\n",
    "* _Restarts_ are needed to mitigate issues due to local optima\n",
    "\n",
    "**And then we can obtain the predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, std = gp.predict(x.reshape(-1,1), return_std=True)\n",
    "xp = pd.Series(index=y.index, data=xp)\n",
    "std = pd.Series(index=y.index, data=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gaussian Processes in scikit-learn\n",
    "\n",
    "**We can now plot the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b7550bd18489bac882fb6ec6f72c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab.plot_gp(target=y, samples=y_tr, pred=xp, std=std, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* We get both a _point estimate_ (the blue line)\n",
    "* ...And a _confidence interval_ (the light blue areas)\n",
    "\n",
    "**But how did we manage that?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "There are _two tricks at work_ here:\n",
    "\n",
    "**1) The training examples $\\bf \\hat{y}_X$ are _part of the GP parameters_**\n",
    "\n",
    "* This is similar to what we have (e.g.) in KDE\n",
    "* ...And quite different from other ML methods (e.g. NN, DTs, SVMs...)\n",
    "\n",
    "Given an new input value $x$, the GP output corresponds to:\n",
    "$$\n",
    "f\\left(y_x \\mid {\\bf \\hat{y}_X}\\right)\n",
    "$$\n",
    "* I.e. the probability of $y_x$ _conditioned on the known examples_\n",
    "\n",
    "**2) For computing $\\Sigma$, we only need to know the _input values_ (i.e. $X$ and $x$)**\n",
    "\n",
    "* This is by construction: all our kernels $K(x_i, x_j)$ are built this way\n",
    "* Which means that we can compute $f\\left(y_x \\mid {\\bf \\hat{y}_X}\\right)$ via a closed-form expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**Thanks to this, for $f\\left(y_x \\mid {\\bf \\hat{y}_X}\\right)$ we can get**\n",
    "\n",
    "The _conditional mean_ (which is also the MAP):\n",
    "$$\n",
    "\\arg\\max_{y_x} f\\left(y_x \\mid {\\bf \\hat{y}_X}\\right)\n",
    "$$\n",
    "* This will be our \"prediction\"\n",
    "\n",
    "The _conditional standard deviation_\n",
    "$$\n",
    "\\sqrt{{\\rm Var} \\left[ f\\left(y_x \\mid {\\bf \\hat{y}_X}\\right)\\right]}\n",
    "$$\n",
    "* This is used to define the confidence intervals\n",
    "\n",
    "**In practice, we GP output is _a probability distribution_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Numeric Example\n",
    "\n",
    "**As an example, say we want a prediction for $x = 2.5$, i.e. $y_{2.5}$**\n",
    "\n",
    "...And let's assume that our training set contains _only one example_\n",
    "\n",
    "* We will consider _separately_ the tenth and first example in our dataset\n",
    "* We have: $(\\hat{x}_9, \\hat{y}_{\\hat{x}_9}) \\simeq (2.01, 2.27)$\n",
    " and $(\\hat{x}_0, \\hat{y}_{\\hat{x}_0}) \\simeq (0.27, 0.58)$\n",
    " \n",
    "**The covariance matrix in the two cases is therefore:**\n",
    "\n",
    "$$\n",
    "\\Sigma_{y_x, \\hat{y}_{\\hat{x}_9}} = \\left(\\begin{array}{cc}\n",
    "K(2.01, 2.01) & K(2.01, 2.5) \\\\\n",
    "K(2.5, 2.01) & K(2.5, 2.5) \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{y_x, \\hat{y}_{\\hat{x}_0}} = \\left(\\begin{array}{cc}\n",
    "K(0.27, 0.27) & K(0.27, 2.5) \\\\\n",
    "K(2.5, 0.27) & K(2.5, 2.5) \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "* Each matrix defines a multivariate Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**Let's actually build the matrices in Python**\n",
    "\n",
    "* Note: scikit-learn kernels are not designed to be used on individual points\n",
    "* So, for this we will rely on basic numpy methods\n",
    "\n",
    "**We start with $\\hat{x}_9$ and $x$, which are _close to each other_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "X9, X0, X = [[x_tr[9]]], [[x_tr[0]]], [[2.5]] # Must be 2D\n",
    "sigma_9x = np.array([[kernel(X9, X9)[0,0], kernel(X9, X)[0,0]],\n",
    "                    [kernel(X, X9)[0,0], kernel(X, X)[0,0]]])\n",
    "f_9x = multivariate_normal([0, 0], cov=sigma_9x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then we do the same for $\\hat{x}_0$ and $x$, which are _far apart_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_0x = np.array([[kernel(X0, X0)[0,0], kernel(X0, X)[0,0]],\n",
    "                    [kernel(X, X0)[0,0], kernel(X, X)[0,0]]])\n",
    "f_0x = multivariate_normal([0, 0], cov=sigma_0x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**$\\hat{x}_9$ and $x$ are _close to each other_, so $\\hat{y}_{\\hat{x}_9}$ and $y_x$ are _strongly correlated_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39baf7b1a6f4138a148910f2f5fd5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yr = np.linspace(-5, 5, 100)\n",
    "nab.plot_distribution_2D(f_9x, yr, yr, figsize=figsize)\n",
    "plt.xlabel('y_{x_9}'); plt.ylabel('y_{x}'); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Still, if we know _neither_ $\\hat{y}_{\\hat{x}_9}$ nor $y_x$, we can only say that they are likely _both zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**But we _do know_ $\\hat{y}_{\\hat{y}_9}$! So, we can use this information_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b9a28d32e4e478ee253ec99b336cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab.plot_distribution_2D(f_9x, yr, yr, figsize=figsize)\n",
    "plt.axvline(10*(y_tr[x_tr[9]] + 5), color='tab:orange');\n",
    "plt.xlabel('y_{x_9}'); plt.ylabel('y_{x}'); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given the observation, the most likely value for $y_{x}$ _changes considerably_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**$\\hat{x}_0$ and $x$ are _far apart_, so $\\hat{y}_{\\hat{x}_0}$ and $y_x$ are _loosely correlated_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843cb097a677483d93303938ee560eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab.plot_distribution_2D(f_0x, yr, yr, figsize=figsize)\n",
    "plt.axvline(10*(y_tr[x_tr[0]] + 5), color='tab:orange');\n",
    "plt.xlabel('y_{x_0}'); plt.ylabel('y_{x}'); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Knowing $\\hat{y}_{\\hat{x}_0}$ is not going to be of much help here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behind the Scenes\n",
    "\n",
    "**So, a few key insight to keep in mind:**\n",
    "\n",
    "* Superficially, GPs behave like _functions that output probability distribution_\n",
    "* Internally, this is enabled by _two components_:\n",
    "   - _The kernel_, defining how all the points are correlated\n",
    "   - _A set of observations_, use to obtain conditional distributions\n",
    "\n",
    "**In scikit-learn:**\n",
    "\n",
    "When we call the `fit` method:\n",
    "\n",
    "* The optimizer adjusts the kernel parameters\n",
    "* ...And _the observations $\\hat{y}_{\\hat{x}}$ are stored_\n",
    "\n",
    "When we call the `predict` method:\n",
    "\n",
    "* The covariance matrix is built\n",
    "* The model computes the conditional distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**Now, let's try to improve our model**\n",
    "\n",
    "We need to _choose a kernel_ appropriate to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d8bc0cfc58464190c8d1cbd6e0baf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab.plot_gp(target=y, samples=y_tr, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We a bit of _noise_, a _period_, and a _trend_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**So, let us deal with the noise first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "kernel = WhiteKernel(0.1, (1e-3, 1e3))\n",
    "kernel += RBF(1, (1e-2, 1e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WhiteKernel` captures the presence of _noise_ in the data\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\sigma^2 \\text{ iff } x_i = x_j, 0 \\text{ otherwise}\n",
    "$$\n",
    "\n",
    "\n",
    "* The only parameter of `WhiteKernel` represents the noise level $\\sigma^2$\n",
    "* A small noise level prevent overfitting the data\n",
    "* ...But too much noise leads to useless predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**It's often a good idea to have magnitude parameters in the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "\n",
    "kernel = WhiteKernel(0.1, (1e-2, 1e2))\n",
    "kernel += ConstantKernel(1, (1e-2, 1e2)) * RBF(1, (1e-2, 1e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConstantKernel` is a constant factor (in this case a relative weight)\n",
    "\n",
    "* ...And allows the optimizer to tune the magnitude of the RBF kernel\n",
    "\n",
    "**Let's repeat training again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteKernel(noise_level=0.01) + 2.21**2 * RBF(length_scale=0.321)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__noise_level is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    }
   ],
   "source": [
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(y_tr.index.values.reshape(-1,1), y_tr.values) # needs 2D input\n",
    "print(gp.kernel_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**Let us see the new predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2bca8de2c046e289d3eb095db7c4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xp, std = gp.predict(x.reshape(-1,1), return_std=True)\n",
    "xp = pd.Series(index=y.index, data=xp)\n",
    "std = pd.Series(index=y.index, data=std)\n",
    "nab.plot_gp(target=y, samples=y_tr, pred=xp, std=std, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Better, but we are still not exploiting the period and the trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**So, let us take them into account, starting with the period**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "kernel = WhiteKernel(0.1, (1e-2, 1e2))\n",
    "kernel += ConstantKernel(1, (1e-2, 1e2)) * RBF(1, (1e-2, 1e2))\n",
    "kernel += ExpSineSquared(1, 1, (1e-2, 1e2), (1e-2, 1e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ExpSineSquared` captures the period:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = e^{-2 \\frac{\\sin^2 \\left(\\pi \\frac{d(x_i,x_j)}{p}\\right)}{l^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The correlation grows is the distance is close to a multiple of the period $p$\n",
    "* The scale parameter $l$ control the rate of decrease/increase\n",
    "* In the implementation, the first parameter is $l$ and the second $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**Now, let's try to capture the trend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "kernel = WhiteKernel(0.1, (1e-2, 1e2))\n",
    "kernel += ConstantKernel(1, (1e-2, 1e2)) * RBF(1, (1e-2, 1e2))\n",
    "kernel += ExpSineSquared(1, 1, (1e-2, 1e2), (1e-2, 1e2))\n",
    "kernel += DotProduct(1, (1e-2, 1e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DotProduct` (somewhat) captures the trend:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\sigma^2 + x_i x_j\n",
    "$$\n",
    "\n",
    "* The larger the $x$ values, the larger the correlation\n",
    "* This allows the distance from the mean (which is zero) to grow\n",
    "* The $\\sigma$ parameter controls the base level of correlation\n",
    "* Unlike all kernels so far `DotProduct` is _not translation-invariant_ (we sa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Improve the Model\n",
    "\n",
    "**The new predictions are a bit better at the edges of the plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteKernel(noise_level=0.01) + 1.17**2 * RBF(length_scale=0.305) + ExpSineSquared(length_scale=2.17, periodicity=0.939) + DotProduct(sigma_0=0.0197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__noise_level is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335889e4d5194e9f99a5c1fdaf9b3365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(y_tr.index.values.reshape(-1,1), y_tr.values) # needs 2D input\n",
    "print(gp.kernel_)\n",
    "xp, std = gp.predict(x.reshape(-1,1), return_std=True)\n",
    "xp = pd.Series(index=y.index, data=xp)\n",
    "std = pd.Series(index=y.index, data=std)\n",
    "nab.plot_gp(target=y, samples=y_tr, pred=xp, std=std, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Considerations\n",
    "\n",
    "**Gaussian Processes are a _very flexible_ ML technique**\n",
    "\n",
    "* They can be used to making predictions\n",
    "* ...Together with their confidence intervals\n",
    "* ...But also for (conditional) density estimation\n",
    "* ...And for generating data\n",
    "\n",
    "**They are _non-trivial to use_:**\n",
    "\n",
    "* In particular, choosing a kernel requires some practice and some understanding\n",
    "* Automating the process is possible, but complex (grid search is likely not enough)\n",
    "\n",
    "**Gaussian processes tend to perform better:**\n",
    "\n",
    "* When _interpolating_ (i.e. predicting values between points in the training set)\n",
    "* ...Rather than when _extrapolating_ (i.e. making predictions far from the training set)\n",
    "* And also when there are only a _few input dimensions_\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
